{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4322, 4)\n",
      "(481, 4)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('myData.csv')\n",
    "y = data[['popularity', 'revenue', 'vote_average', 'vote_count']].values\n",
    "x = data.drop(columns=['popularity', 'revenue', 'vote_average', 'vote_count'])\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=5233)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numTextSplit(data):\n",
    "    numData = data.drop(columns=['text']).values\n",
    "    textData = data['text'].values\n",
    "    return numData, textData\n",
    "x_train_num, x_train_text = numTextSplit(x_train)\n",
    "x_test_num, x_test_text = numTextSplit(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeInputData(numData, textData, labels, isTrain):\n",
    "    MAX_LEN = 30\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    tokenized_text = [tokenizer.tokenize(text) for text in textData]\n",
    "    ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_text],\n",
    "                              maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "    # Create attention masks\n",
    "    attention_masks = []\n",
    "    # Create a mask of 1s for each token followed by 0s for padding\n",
    "    for seq in ids:\n",
    "        seq_mask = [float(i>0) for i in seq]\n",
    "        attention_masks.append(seq_mask)\n",
    "\n",
    "    # Convert all of our data into torch tensors, the required datatype for our model\n",
    "    textInputs = torch.tensor(ids)\n",
    "    masks = torch.tensor(attention_masks)\n",
    "    numInputs = torch.tensor(numData)\n",
    "    labels = torch.tensor(labels)\n",
    "    # Select a batch size for training. \n",
    "    batch_size = 1\n",
    "\n",
    "    # Create an iterator of our data with torch DataLoader \n",
    "    dataset = TensorDataset(textInputs, masks, numInputs, labels)\n",
    "    if isTrain:\n",
    "        sampler = RandomSampler(dataset)\n",
    "    else:\n",
    "        sampler = SequentialSampler(dataset)\n",
    "        \n",
    "    dataloader = DataLoader(dataset, sampler=sampler, batch_size=batch_size)\n",
    "    return dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLoader = makeInputData(numData=x_train_num, textData=x_train_text, labels=y_train, isTrain=True)\n",
    "testLoader = makeInputData(numData=x_test_num, textData=x_test_text, labels=y_test, isTrain=False)\n",
    "\n",
    "del x_train, x_test, x, y, data, x_train_num, x_train_text, x_test_num, x_test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "import torchvision.models\n",
    "import hiddenlayer as hl\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.bert = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=5)\n",
    "        self.fc1 = nn.Linear(7, 5)\n",
    "        self.output = nn.Linear(5,4)\n",
    "        \n",
    "    def forward(self, ids, mask, numbers):\n",
    "        x1 = self.bert(ids, token_type_ids=None, attention_mask=mask)\n",
    "        x2 = numbers\n",
    "        x = torch.cat((x1.float(), x2.float()), dim=1)\n",
    "#         x = nn.ReLU(self.fc1(x))\n",
    "        x = self.fc1(x)\n",
    "        x = self.output(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n",
      "\n",
      "\n",
      "\n",
      "Epoch:   0%|                                                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.007056371035072279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files (x86)\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  25%|█████████████████▌                                                    | 1/4 [3:00:10<9:00:32, 10810.96s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.007303820847367371\n",
      "已花時間: 180m 11s\n",
      "--------------------------------------------------\n",
      "Train loss: 0.00671535229344939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch:  50%|███████████████████████████████████                                   | 2/4 [6:00:53<6:00:40, 10820.41s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.007969603283362745\n",
      "已花時間: 360m 54s\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "since = time.time()\n",
    "\n",
    "optimizer = BertAdam(model.parameters(),\n",
    "                     lr=2e-5,\n",
    "                     warmup=.1)\n",
    "\n",
    "criterion =  nn.MSELoss()\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "  \n",
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "# Number of training epochs \n",
    "epochs = 4\n",
    "\n",
    "# BERT training loop\n",
    "for _ in trange(epochs, desc=\"Epoch\"):  \n",
    "  \n",
    "  ## TRAINING\n",
    "  \n",
    "    # Set our model to training mode\n",
    "    model.train()  \n",
    "    # Tracking variables\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    # Train the data for one epoch\n",
    "    for step, batch in enumerate(trainLoader):\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_input_number, b_labels = batch\n",
    "        # Clear out the gradients (by default they accumulate)\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "            \n",
    "        preds = model(ids=b_input_ids.long(), mask=b_input_mask, numbers=b_input_number)\n",
    "        loss = criterion(preds, b_labels.float())\n",
    "        \n",
    "        train_loss_set.append(loss.item())    \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "        # Update tracking variables\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "#         break\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    \n",
    "       \n",
    "    ## VALIDATION\n",
    "\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "    # Tracking variables \n",
    "    eval_loss = 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    # Evaluate data for one epoch\n",
    "    for step, batch in enumerate(testLoader):\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_input_number, b_labels = batch\n",
    "        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "          # Forward pass, calculate logit predictions\n",
    "            logits = model(ids=b_input_ids.long(), mask=b_input_mask, numbers=b_input_number)    \n",
    "            logits = torch.tensor(logits)\n",
    "            loss = criterion(logits, b_labels.float())\n",
    "            eval_loss += loss.item()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        nb_eval_steps += 1\n",
    "#         break\n",
    "    print('Test Loss: {}'.format(eval_loss/nb_eval_steps))\n",
    "    td = time.time() - since\n",
    "    print('已花時間: {:.0f}m {:.0f}s'.format(td // 60, td % 60))\n",
    "    print('-'*50)\n",
    "\n",
    "    \n",
    "print('結束訓練')\n",
    "time_elapsed = time.time() - since\n",
    "print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "# plot training performance\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(train_loss_set)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
